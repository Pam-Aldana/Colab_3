{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOIYnR48tn6ezAh6nPij4K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tXiFl4cw1Y2T"},"outputs":[],"source":["# Insertar cuantos bloques de código y markdown consideren necesarios\n","import pandas as pd# Estructura y manipulación de datos\n","import seaborn as sns\n","from google.colab import files\n","import numpy as np # Programación vectorial\n","from seaborn import boxplot\n","from numpy import array\n","from scipy import stats\n","\n","# Gráficos\n","\n","import matplotlib.pyplot as plt # Visualización\n","from matplotlib import style\n","\n","\n","# Preprocesado y modelado\n","from scipy.stats import pearsonr\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_squared_error\n","#import statsmodels.api as sm\n","#import statsmodels.formula.api as smf\n","# Configuración warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Configuración matplotlib\n","plt.rcParams['image.cmap'] = \"bwr\"\n","#plt.rcParams['figure.dpi'] = \"100\"\n","plt.rcParams['savefig.bbox'] = \"tight\"\n","style.use('ggplot') or plt.style.use('ggplot')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#rt =\"/content/drive/MyDrive/Base_clientes_Monopoly THIS.xlsx\"\n","rt=\"/content/drive/MyDrive/Base_clientes_Monopoly-0.xlsx\"# poner la ruta de tu monopoli.xlsx que tienes en tu drive\n","\n","df = pd.read_excel(rt)\n","df.head(5)"],"metadata":{"id":"XfQnPMba1oRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#uploaded = files.upload()\n","#for fn in uploaded.keys():\n","#    name = fn\n","#df = pd.read_excel(name)  # Usa read_excel para leer archivos de Excel\n","#df.head(5)\n"],"metadata":{"id":"6ZGdjvda1oNr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fase 2 Comprender los datos\n","\n","Analisis exploratorio de datos :EDA\n","\n"],"metadata":{"id":"ThL4StoD1zEg"}},{"cell_type":"code","source":["df.head(10)"],"metadata":{"id":"pHO9PmvK1oGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Asegurarte de que pandas muestre toda la información de las columnas\n","pd.set_option('display.max_columns', None)\n","\n","# Mostrar toda la información de las variables (columnas)\n","df.info()\n"],"metadata":{"id":"o2AsC3PX1n7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TIPO DE DATOS\n","df.info"],"metadata":{"id":"CJ02K0n32IfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dimensiones DEL CONJUNTO DE DATOS\n","df.shape()"],"metadata":{"id":"j4R9qyzv2Ib6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DATOS FALTANTES"],"metadata":{"id":"yEoYS0xI2ghL"}},{"cell_type":"code","source":["df.isna().sum().sort_values(ascending=False)\n","#EN ORDEN DESCENDENTE"],"metadata":{"id":"niig2ABx2IY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#total nulos\n","df.isna().sum().sum()"],"metadata":{"id":"SeIhj2Mr2IV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Estadísticas descriptivas**\n","\n"," **Variables numéricas**  Estadísticas descriptivas : la media, desviación estándar, mínimo, cuartiles y máximo para columnas numéricas."],"metadata":{"id":"ZfXUvEaB200u"}},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"fu_imXXD2ISs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Medidas de tendencia central\n","(moda/mediana/media)"],"metadata":{"id":"lJUqiWwZ2_QB"}},{"cell_type":"code","source":["# Seleccionar solo las columnas numéricas\n","numeric_df = df.select_dtypes(include=[np.number])\n","\n","# Obtener medidas de tendencia central para todas las columnas numéricas\n","mean = numeric_df.mean()\n","median = numeric_df.median()\n","mode = numeric_df.mode().iloc[0]  # La primera fila contiene la moda\n","\n","# Crear un DataFrame con las medidas\n","summary_df = pd.DataFrame({\n","    'Media': mean,\n","    'Mediana': median,\n","    'Moda': mode\n","})\n","\n","# Mostrar la tabla con medidas de tendencia central\n","summary_df.T"],"metadata":{"id":"6m9uQ7_s2IPb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Medidas de Dispersion\n","\n","\n","Rango: La diferencia entre el valor máximo y el valor mínimo.\n","Varianza: La media de las diferencias al cuadrado entre cada valor y la media.\n","Desviación Estándar: La raíz cuadrada de la varianza. Indica cuánto se dispersan los valores respecto a la media.\n","Coeficiente de Variación: La desviación estándar dividida por la media, expresada como porcentaje."],"metadata":{"id":"CT2BWMP_3JkT"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","# Seleccionar solo las columnas numéricas\n","numeric_df = df.select_dtypes(include=[np.number])\n","\n","# Obtener medidas de dispersión para todas las columnas numéricas\n","std_dev = numeric_df.std()  # Desviación estándar\n","variance = numeric_df.var()  # Varianza\n","range_ = numeric_df.max() - numeric_df.min()  # Rango (max - min)\n","iqr = numeric_df.quantile(0.75) - numeric_df.quantile(0.25)  # Rango intercuartílico (IQR)\n","mean = numeric_df.mean()  # Media\n","\n","# Calcular el Coeficiente de Variación (CV)\n","cv = (std_dev / mean) * 100\n","\n","# Crear un DataFrame con las medidas\n","dispersion_df = pd.DataFrame({\n","    'Desviación Estándar': std_dev,\n","    'Varianza': variance,\n","    'Rango': range_,\n","    'Rango Intercuartílico': iqr,\n","    'Coeficiente de Variación (%)': cv\n","})\n","\n","# Mostrar la tabla con medidas de dispersión\n","dispersion_df.T\n"],"metadata":{"id":"1zDIvqtq3JNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["medidas de forma : kurtosis, asimetria"],"metadata":{"id":"bcF4nh653g7Z"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.stats import kurtosis, skew\n","\n","# Seleccionar solo las columnas numéricas\n","numeric_df = df.select_dtypes(include=[np.number])\n","\n","# Calcular medidas de forma\n","skewness = numeric_df.apply(lambda x: skew(x.dropna()))\n","kurtosis_values = numeric_df.apply(lambda x: kurtosis(x.dropna()))\n","\n","# Crear un DataFrame con las medidas de forma\n","shape_df = pd.DataFrame({\n","    'Asimetría (Skewness)': skewness,\n","    'Curtosis': kurtosis_values\n","})\n","\n","# Mostrar la tabla con medidas de forma\n","shape_df.T"],"metadata":{"id":"n7YmBzm02IMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"PwN7fk063ubN"}},{"cell_type":"markdown","source":["##**Analisis Distribucion de las variables Numericas**"],"metadata":{"id":"mmHyxrWA3uJX"}},{"cell_type":"markdown","source":["Visualizamos las distribucion de las variable para identificar posibles sesgos, y observar características como la normalidad, la presencia de valores atípicos y la dispersión de los datos.Lo que nos ayuda a elegir el método de normalización de características mas optimo.\n","Identificar que variables tienen una distribución normal, se debe observar la media y la desviación estándar, así como considerar la simetría de los datos.\n","\n","Simetría: La distribución es simétrica alrededor de la media.\n","Media y mediana iguales: La media y la mediana son aproximadamente iguales."],"metadata":{"id":"iOCwk1dq3x-e"}},{"cell_type":"code","source":["numeric_columns = df.select_dtypes(include=[np.number]).columns\n","numeric_columns"],"metadata":{"id":"tP4VAziR2IHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Obtener todas las columnas numéricas\n","numeric_columns = df.select_dtypes(include=[np.number]).columns\n","\n","# Configurar el tamaño de las figuras y el número de variables por página\n","num_vars_per_page = 10  # Ajusta esto según el número de gráficos que quieras por página\n","num_pages = (len(numeric_columns) + num_vars_per_page - 1) // num_vars_per_page\n","\n","for page in range(num_pages):\n","    plt.figure(figsize=(16, num_vars_per_page * 4))\n","\n","    # Seleccionar variables para esta página\n","    start = page * num_vars_per_page\n","    end = min(start + num_vars_per_page, len(numeric_columns))\n","    page_vars = numeric_columns[start:end]\n","    # Crear un gráfico de distribución para cada variable en la página\n","    for i, var in enumerate(page_vars):\n","        plt.subplot(num_vars_per_page, 2, i + 1)  # Crear una cuadrícula con num_vars_per_page filas y 2 columnas\n","        sns.histplot(df[var].dropna(), kde=True, bins=30)\n","        plt.title(f'Distribución de {var}')\n","        plt.xlabel(var)\n","        plt.ylabel('Frecuencia')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"nKUdRfoT2IBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variable : Sexo\n","#Total hombres y mujeres\n","\n","total_m = 0\n","total_h = 0\n","for cantidad in df['Sexo']:\n","    if cantidad == 'M':\n","        total_m += 1\n","    elif cantidad == 'H':\n","        total_h += 1\n","print(f\" {total_m} mujeres  y {total_h} hombres\")"],"metadata":{"id":"whWhR44P2Hyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Grafico de barras de Cantidad de cuentas por cliente\n","\n","plt.figure(figsize=(8, 7))\n","sns.countplot(data=df, x='Cuentas')\n","plt.xlabel('Cantidad de cuentas por cliente')\n","plt.ylabel('Clientes')\n","plt.title('Grafico de barras de Cantidad de cuentas por cliente')\n","plt.show()"],"metadata":{"id":"YIJJpDjP4Lqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Graficos de analisis distribucion\n","\n","variables_num = ['Renta', 'Edad', 'TC', 'Fac_T12', 'UsoL1_T12', 'CUPO_L1']\n","for variable in variables_num:\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    sns.histplot(data=df, x=variable, hue='target', bins=30, kde=True)\n","    plt.xlabel(variable)\n","    plt.ylabel('Frecuencia')\n","    plt.title(f'Distribución de {variable} para clientes que pagan y no')\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"Fo3zgZUq5Ffy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Boxplot de edad\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(y='Edad', data=df)\n","plt.title('Distribución de la Edad')\n","plt.show()"],"metadata":{"id":"DJu5Y1N35Mda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Scatter plot edad - renta: relacion entre 2 variables\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(x='Edad', y='Renta', data=df)\n","plt.title('Relación entre Edad y Renta')\n","plt.xlabel('Edad')\n","plt.ylabel('Renta')\n","plt.show()"],"metadata":{"id":"--eZIA5y5QGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Violin plot edad - sexo\n","plt.figure(figsize=(10, 6))\n","sns.violinplot(x='Sexo', y='Edad', data=df)\n","plt.title('Distribución de la Edad por Sexo')\n","plt.xlabel('Sexo')\n","plt.ylabel('Edad')\n","plt.show()"],"metadata":{"id":"NDiZn_Y75bOP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["escalado de caracteristicas"],"metadata":{"id":"1dO7SqOr5diF"}},{"cell_type":"markdown","source":["Estandarización:\n","Ajusta los datos para que tengan una media de 0 y una desviación estándar de 1. Utiliza StandardScaler para las columnas con distribución normal.\n","\n","La variable que tiende a distribucion normal es edad, para aplicar estandarizacion.\n","\n","Normalización:\n","Ajusta los datos para que se encuentren en un rango específico, generalmente entre 0 y 1. Utiliza MinMaxScaler para las columnas con distribución no normal.\n","\n","Normalizaremos las características para que las características que tienen no tiene una distribucion normal."],"metadata":{"id":"fxfffIi_5jV6"}},{"cell_type":"markdown","source":["CORRELACION ⁉\n","\n","Se analiza la correlación que existe en las variables entre si,  teniendo en cuenta que aunque exista correlación no necesariamente implica que exista causalidad.<br>\n","Se observa que existen correlaciones tanto positivas como negativas."],"metadata":{"id":"1sqv1Imf5wtw"}},{"cell_type":"code","source":["#CORRELACION\n","corr = df.corr(numeric_only=True)\n","corr"],"metadata":{"id":"oYG38LGY5bK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Mapa de correlacion\n","\n","columnas_heatmap = ['Subsegmento','Region','Edad','Renta','Antiguedad','Internauta','Adicional','Dualidad','Monoproducto','Ctacte','Consumo', 'Hipotecario','Debito','Cuentas','TC','CUPO_L1','CUPO_MX','CUPO_L2','Col_T12','ColL1TE_T12','EeccInt_T12','EeccNac_T12','Fac_T12','FacAI_T12','UsoL1_T12', 'UsoL2_T12','target']\n","df_heatmap = df[columnas_heatmap]\n","matriz_correlacion = df_heatmap.corr()\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(matriz_correlacion, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Mapa de Correlación')\n","plt.show()"],"metadata":{"id":"trnZiCoS5bI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_corr = corr['Renta'].sort_values(ascending=False)[1:16]\n","print(top_corr)"],"metadata":{"id":"hm0NjI5l5bGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dttgdiTk5bBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**OUTLIERS**"],"metadata":{"id":"f0rNjYWi54Wm"}},{"cell_type":"code","source":["#Analisis de outliers con Z-score\n","\n","numeric_cols = df.select_dtypes(include=[np.number]).columns\n","z_scores = np.abs(stats.zscore(df[numeric_cols]))\n","#Z-score > 3\n","outliers = np.where(z_scores > 3)\n","print(\"Filas con outliers:\", outliers[0])\n","print(\"Columnas con outliers:\", outliers[1])\n","outlier_rows = df.iloc[outliers[0]]\n","outlier_rows"],"metadata":{"id":"Z7s8AcFM57XE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ANALIS DE OUTLIERS CON IQR\n","#Obtener todas las columnas numericas\n","numeric_columns = df.select_dtypes(include=[np.number]).columns\n","for column in df[numeric_columns].columns:\n","        Q1 = df[column].quantile(0.25)\n","        Q3 = df[column].quantile(0.75)\n","\n","        IQR = Q3 - Q1\n","\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n","\n","        outliers_count = outliers.sum()\n","        outliers_percentage = outliers.mean() * 100\n","\n","        print(f\"Columna '{column}' tiene {outliers_count} outliers.\")\n","        print(f\"Porcentaje de outliers: {outliers_percentage:.2f}%\")\n","        print(\"---\")\n","\n","total_outliers = sum(outliers.sum() for column in df.columns)\n","total_entries = len(df) * len(df.columns)\n","total_percentage = total_outliers / total_entries * 100\n","\n","print(f\"Porcentaje total de outliers en todo el dataset: {total_percentage:.2f}%\")\n","print(f\"Cantidad total de outliers en todo el dataset: {total_outliers}\")\n"],"metadata":{"id":"wSBbFsPK6BCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DIAGRAMA DE CAJAS DE TODAS LAS VARIABLES NUMERICAS\n","numeric_columns = df.select_dtypes(include=[np.number]).columns\n","numeric_columns\n","plt.figure(figsize=(10,25))\n","df.boxplot(column=['Subsegmento', 'Region', 'Edad', 'Renta', 'Antiguedad', 'CUPO_L1', 'CUPO_L2', 'CUPO_MX', 'Fac_T12','FacCN_T12','FacCCOT_T12', 'FacDebCom_T12', 'TxsDebCom_T12', 'FacDebAtm_T12', 'TxsDebAtm_T12', 'Col_T12', 'ColL1T0_T12', 'ColL1TE_T12', 'ColL2T0_T12', 'ColL2CC_T12', 'PagoNac_T12', 'EeccNac_T12','UsoL1_T12', 'UsoL2_T12', 'UsoLI_T12']\n",", vert=False)\n","plt.show()"],"metadata":{"id":"3U8sof7f6KGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"RCSoTcgp6XUK"}},{"cell_type":"markdown","source":["## **Estadísticas descriptivas**\n","**Variables Categoricas**\n","\n","- COUNT : Cantidad de datos en la Variable\n","- UNIQUE : Esto representa la cantidad de datos unicos en la columna.\n","- TOP : Es el valor mas frecuente de la columna\n","- FREQ : Este es el numero de veces que se repite el valor de TOP"],"metadata":{"id":"TDrjfTd46ZGz"}},{"cell_type":"code","source":["# Seleccionar solo las columnas categóricas\n","categorica_df = df.select_dtypes(include=['object', 'category'])\n","\n","# Mostrar estadísticas descriptivas de las columnas categóricas\n","categorica_df.describe()\n"],"metadata":{"id":"epzAW27c6KDS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Grafico de barra**\n","\n","Se muestra la frecuencia de cada clase en la respectiva columna ,  permiten visualizar rápidamente qué clase son más comunes en cada variable categórica.\n","- Tambien permite ver el balanceo de las clases."],"metadata":{"id":"wtuWq_w66ouP"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Seleccionar solo las columnas categóricas\n","categorica_df = df.select_dtypes(include=[object])\n","\n","# Crear un gráfico de barras para cada columna categórica\n","for column in categorica_df.columns:\n","    plt.figure(figsize=(12, 6))\n","\n","    # Contar la frecuencia de cada clase\n","    counts = categorica_df[column].value_counts()\n","\n","    # Crear el gráfico de barras\n","    sns.barplot(x=counts.index, y=counts.values, palette='viridis')\n","\n","    plt.title(f'Frecuencia de cada clase en la columna {column}')\n","    plt.xlabel('Clases')\n","    plt.ylabel('Frecuencia')\n","    plt.xticks(rotation=90)  # Rotar etiquetas del eje x si hay muchas clases\n","    plt.show()\n"],"metadata":{"id":"az5naOfd6J-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Las clases de las variables : estan desbalanceadas, significa que una de las clases está mucho más frecuente que la otra. Esto puede afectar la capacidad del  modelo para aprender correctamente las relaciones entre las características y las etiquetas. Se pueden aplicar tecnicas para manejar estas clases, como SMOTE, oversampling, undersampling."],"metadata":{"id":"-jCzIU_p6042"}},{"cell_type":"markdown","source":["**Datos Faltantes**"],"metadata":{"id":"vVZvzqws653T"}},{"cell_type":"markdown","source":["Errores tipográficos en variables categóricas\n","En una variable categórica pueden aparecer sub-niveles.Se deben unificar"],"metadata":{"id":"2T4Wbp_V69zd"}},{"cell_type":"code","source":["# Seleccionar solo las columnas categóricas\n","categorica_df = df.select_dtypes(include=[object])\n","categorica_df.info()\n","#Separo las variables por tipo y reviso los datos para ver si se repiten o se ve algo anormal\n","\n","for col in categorica_df:\n","  print(f'Columna {col}: {df[col].unique()}{df[col].nunique()} subniveles')"],"metadata":{"id":"j5xcgLId7D8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Analisis de las variables objetivo**\n"],"metadata":{"id":"BhLcpYKB8f7Y"}},{"cell_type":"code","source":["df['target'].value_counts()"],"metadata":{"id":"mgEZePKr8l8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['target'].isnull().sum()"],"metadata":{"id":"lqKqyMyg8n6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#GRAFICO DE barras\n","plt.figure(figsize=(4, 4))\n","df['target'].value_counts().plot(kind='bar')\n","# Agrega etiquetas y título para hacer el gráfico más informativo.\n","plt.xlabel('target')\n","plt.ylabel('Frecuencia')\n","plt.title('Gráfico de Barras de target')\n","# Muestra el gráfico de barras en una ventana gráfica\n","plt.show()\n","\n","\n"],"metadata":{"id":"KX3IwMKo8qc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configurar el tamaño del gráfico\n","plt.figure(figsize=(4, 4))\n","\n","# Graficar la distribución de frecuencias de la columna 'EeccNac_T07'\n","df_regresion['EeccNac_T07'].value_counts().plot(kind='bar')\n","\n","# Agregar etiquetas y título al gráfico\n","plt.xlabel('EeccNac_T07')\n","plt.ylabel('Frecuencia')\n","plt.title('Gráfico de Barras de target')\n","\n","# Mostrar el gráfico\n","plt.show()\n"],"metadata":{"id":"NZhl0fJVHtHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_regresion['EeccNac_T07'].describe()\n","\n"],"metadata":{"id":"Yt0-uQ3gIK2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Esto eliminará todas las filas donde EeccNac_T07 tiene valores nulos.#\n","df_regresion_clean = df_regresion.dropna(subset=['EeccNac_T07'])\n"],"metadata":{"id":"-1MvaXZiJkq1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TARGET NUMERICO"],"metadata":{"id":"sXmLOfJq83sy"}},{"cell_type":"code","source":["# NULOS DE LA VARIABLE TARGET NUMERICO\n","df['CUPO_MX'].isnull().sum()"],"metadata":{"id":"NCbwRRyL8yc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.CUPO_MX.value_counts()"],"metadata":{"id":"xsZQbQDE863l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['CUPO_MX'].describe()"],"metadata":{"id":"Mo_1LylR88SK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['CUPO_MX'].mode()"],"metadata":{"id":"qUlRuVEw88Os"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CORRELACION CON EL TARGET NUMERICO"],"metadata":{"id":"Wd-BgKm59Dgs"}},{"cell_type":"code","source":["# Filtrar solo las columnas numéricas del DataFrame\n","df_numeric = df.select_dtypes(include=['float64', 'int64'])\n","\n","# Calcular la matriz de correlación solo para las columnas numéricas\n","corr = df_numeric.corr()\n","\n","# Seleccionar las 15 variables más correlacionadas con 'CUPO_MX', excluyendo a sí misma\n","top_corr = corr['CUPO_MX'].drop('CUPO_MX').sort_values(ascending=False).head(30)\n","\n","# Mostrar las correlaciones\n","print(top_corr)\n"],"metadata":{"id":"j-hZj_gh88MJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Boxplot de variables de interes\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(y='CUPO_MX', data=df)\n","plt.title('Distribución de ...')\n","plt.show()"],"metadata":{"id":"d5EyfKbB88Jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Mapa de correlacion de variables de interes\n","columnas_heatmap = ['NombreVar','NombreVar']\n","df_heatmap = df[columnas_heatmap]\n","matriz_correlacion = df_heatmap.corr()\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(matriz_correlacion, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Mapa de Correlación')\n","plt.show()"],"metadata":{"id":"w_CfqfAd88Gq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_discriminado# se crea un nuevo df\n","df_discriminado = df[['Renta', 'CUPO_L1']]"],"metadata":{"id":"oLj9NljF87_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tipo de dato del target\n","df['target'].dtype"],"metadata":{"id":"cUO0Yygn_YAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y2yJCbkI_YoV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## outliers"],"metadata":{"id":"sjsTcTNw-vZy"}},{"cell_type":"code","source":["#Obtener todas las columnas numericas\n","numeric_columns = df.select_dtypes(include=[np.number]).columns\n","for column in df[numeric_columns].columns:\n","        Q1 = df[column].quantile(0.25)\n","        Q3 = df[column].quantile(0.75)\n","\n","        IQR = Q3 - Q1\n","\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n","\n","        outliers_count = outliers.sum()\n","        outliers_percentage = outliers.mean() * 100\n","\n","        print(f\"Columna '{column}' tiene {outliers_count} outliers.\")\n","        print(f\"Porcentaje de outliers: {outliers_percentage:.2f}%\")\n","        print(\"---\")\n","\n","total_outliers = sum(outliers.sum() for column in df.columns)\n","total_entries = len(df) * len(df.columns)\n","total_percentage = total_outliers / total_entries * 100\n","\n","print(f\"Porcentaje total de outliers en todo el dataset: {total_percentage:.2f}%\")\n","print(f\"Cantidad total de outliers en todo el dataset: {total_outliers}\")"],"metadata":{"id":"hvuPYOmq-wyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Seleccionar las columnas específicas\n","columnas_interes = ['Renta', 'EeccNac_T07']\n","\n","# Inicializar el contador total de outliers\n","total_outliers = 0\n","\n","# Iterar sobre las columnas de interés\n","for column in columnas_interes:\n","    Q1 = df[column].quantile(0.25)  # Primer cuartil (Q1)\n","    Q3 = df[column].quantile(0.75)  # Tercer cuartil (Q3)\n","\n","    IQR = Q3 - Q1  # Rango intercuartílico (IQR)\n","\n","    # Calcular límites inferior y superior para outliers\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Identificar outliers\n","    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n","\n","    # Contar los outliers en la columna actual\n","    outliers_count = outliers.sum()\n","    outliers_percentage = outliers.mean() * 100\n","\n","    # Acumular el total de outliers\n","    total_outliers += outliers_count\n","\n","    # Mostrar los resultados para la columna actual\n","    print(f\"Columna '{column}' tiene {outliers_count} outliers.\")\n","    print(f\"Porcentaje de outliers: {outliers_percentage:.2f}%\")\n","    print(\"---\")\n","\n","# Calcular el total de entradas\n","total_entries = len(df) * len(columnas_interes)\n","\n","# Calcular el porcentaje total de outliers\n","total_percentage = total_outliers / total_entries * 100\n","\n","# Mostrar los resultados generales\n","print(f\"Porcentaje total de outliers en las columnas seleccionadas: {total_percentage:.2f}%\")\n","print(f\"Cantidad total de outliers en las columnas seleccionadas: {total_outliers}\")\n"],"metadata":{"id":"aJVDghCrC8F6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,25))\n","df.boxplot(column=['Subsegmento', 'Region', 'Edad', 'Renta', 'Antiguedad', 'CUPO_L1', 'CUPO_L2', 'CUPO_MX', 'Fac_T12','FacCN_T12','FacCCOT_T12', 'FacDebCom_T12', 'TxsDebCom_T12', 'FacDebAtm_T12', 'TxsDebAtm_T12', 'Col_T12', 'ColL1T0_T12', 'ColL1TE_T12', 'ColL2T0_T12', 'ColL2CC_T12', 'PagoNac_T12', 'EeccNac_T12','UsoL1_T12', 'UsoL2_T12', 'UsoLI_T12']\n",", vert=False)\n","plt.show()"],"metadata":{"id":"sIep3acR-zXt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FASE 3 : PREPARACION DE LOS DATOS"],"metadata":{"id":"oW_8DG6n_usc"}},{"cell_type":"markdown","source":["**transformacion logaritmica**\n","\n","se aplica cuando los datos están sesgados positivamente, es decir, cuando la distribución tiene una larga cola hacia la derecha (valores extremadamente grandes). Esta técnica se usa para reducir la asimetría y hacer que los datos se acerquen más a una distribución normal, para  mejorar la interpretación y los resultados de ciertos modelos\n","\n","Indicadores de cuándo aplicar una transformación logarítmica:\n","Sesgo positivo: Si los datos tienen un sesgo positivo significativo (con más valores concentrados en el lado inferior de la distribución y una cola larga hacia la derecha), la transformación logarítmica puede ayudar a reducir este sesgo.\n","\n","Valores extremadamente grandes (outliers): Si los datos contienen valores muy grandes que están distorsionando la escala de la variable, una transformación logarítmica puede comprimir estos valores y hacer que los outliers sean menos influyentes.\n","\n","Distribución sesgada o con cola larga: Al observar la distribución de la variable, si ves una cola larga que se extiende hacia valores altos, esta es una señal clara de que la transformación logarítmica podría ser beneficiosa.\n","\n","Heterocedasticidad: Si en un análisis de regresión los errores no son constantes (heterocedasticidad), la transformación logarítmica puede estabilizar la varianza.\n","\n","Modelos que asumen normalidad:  como la regresión lineal, asumen que los datos tienen una distribución normal."],"metadata":{"id":"Bt_t4LesG78g"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Asegúrate de que los valores no sean cero o negativos antes de la transformación\n","df['Renta_log'] = np.log(df['Renta'].replace(0, np.nan)).fillna(0)\n","df['EeccNac_T07_log'] = np.log(df['EeccNac_T07'].replace(0, np.nan)).fillna(0)\n"],"metadata":{"id":"VDo5vp2IH1ln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Precauciones:\n","Valores negativos o cero: La transformación logarítmica solo es válida para valores mayores que cero. Si tienes valores cero o negativos en tus datos, puedes:\n","\n","Eliminar esos valores si son pocos.\n","Transformar los valores negativos o cero añadiendo un valor constante a todos los datos. Por ejemplo, sumar una pequeña cantidad (como 1) a la variable para evitar tomar logaritmos de cero:"],"metadata":{"id":"LUhpJleUHrHp"}},{"cell_type":"markdown","source":["Reversibilidad: Si transformas los datos logarítmicamente, debes recordar que para interpretar los resultados o revertir la transformación, deberás aplicar la exponencial inversa (np.exp())."],"metadata":{"id":"Qu7MRSaAHnIQ"}},{"cell_type":"markdown","source":["Escalado de variables"],"metadata":{"id":"QCH65Iy-_47b"}},{"cell_type":"code","source":["#Opciones de codigo para fase 3\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, SimpleImputer, KNNImputer, LabelEncoder\n","from sklearn.impute import KNNImputer\n","from sklearn.decomposition import PCA\n","\n","\n","\n","#Logaritmo natural (normalizacion)\n","df['NombreVar'] = np.log1p(df['NombreVar'])\n","\n","#Elevacion al cuadrado (normalizacion)\n","df['NombreVar'] = df['NombreVar'] ** 2\n","\n","\n","\n","#Min-Max Scaling (escalamiento)\n","scaler_min_max = MinMaxScaler()\n","df[['NombreVar', 'NombreVar2', 'NombreVar3']] = scaler_min_max.fit_transform(df[['NombreVar', 'NombreVar2', 'NombreVar3']])\n","\n","#Estandarizacin (escalamiento)\n","scaler_standard = StandardScaler()\n","df[['NombreVar', 'NombreVar2', 'NombreVar3']] = scaler_standard.fit_transform(df[['NombreVar', 'NombreVar2', 'NombreVar3']])\n","\n","#Robust Scaling (escalamiento)\n","scaler_robust = RobustScaler()\n","df[['NombreVar', 'NombreVar2']] = scaler_robust.fit_transform(df[['NombreVar', 'NombreVar2']])\n","\n","\n","\n","#Imputacion por mediana\n","imputer_median = SimpleImputer(strategy='median')\n","df[['NombreVar', 'NombreVar2', 'NombreVar3']] = imputer_median.fit_transform(df[['NombreVar', 'NombreVar2', 'NombreVar3']])\n","\n","#Imputacion por KNN\n","imputer_knn = KNNImputer(n_neighbors=5)\n","df[['NombreVar', 'NombreVar2']] = imputer_knn.fit_transform(df[['NombreVar', 'NombreVar2']])\n","\n","#Imputacion de valores extremos\n","df.fillna(df.mode().iloc[0], inplace=True)\n","\n","#Eliminacion de outliers basado en Z-Score\n","z_scores = np.abs(stats.zscore(df[['NombreVar', 'NombreVar2', 'NombreVar3']]))\n","df = df[(z_scores < 3).all(axis=1)]\n","\n","#Eliminar la columna Unnamed: 574 (agregar las columnas que sea necesario)\n","df = df.drop(columns=['Unnamed: 574'], errors='ignore')\n","\n","#Tratar outliers en variables (con los percentiles)\n","for col in ['NombreVar', 'NombreVar2']:\n","    q_low = df[col].quantile(0.01)\n","    q_high = df[col].quantile(0.99)\n","    df = df[(df[col] > q_low) & (df[col] < q_high)]\n","\n","\n","\n","#Convertir los categoricos en numericos\n","label_encoder = LabelEncoder()\n","df['NombreVar'] = label_encoder.fit_transform(df['NombreVar'])\n","df['NombreVar2'] = label_encoder.fit_transform(df['NombreVar2'])\n","\n","\n","\n","#Eliminar variables altamente correlacionas que son irrelevantes para nuestro target???\n","\n","\n","\n","#Reduccion de dimensionalidad con PCA ??????\n","from sklearn.decomposition import PCA\n","#Var a las que le aplicaremos PCA para reducirle la dimensionalidda\n","columnas_pca = ['NombreVar', 'NombreVar2', 'NombreVar3']\n","#Escalar\n","scaler = StandardScaler()\n","X_escalado = scaler.fit_transform(df[columnas_pca])\n","#PCA\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_escalado)\n","#Crear nuevo df y añadirle las var con pca\n","df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n","#Agregar columnas del df original al df nuevo que tiene var con pca\n","columnas_adicionales = ['NombreVar1', 'NombreVar2']\n","#Crear un df con todas las var que necesitams\n","df_adicionales = df[columnas_adicionales]\n","#Combinar todo\n","df_final = pd.concat([df_adicionales, df_pca], axis=1)\n","#Imprimir primeras filas\n","print(df_final.head())\n","\n","\n","\n","\n","\n"],"metadata":{"id":"S_NsYPNP_stj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**1- Imputación simple: datos faltantes completamente aleatorios**\n","\n","Se utiliza una única regla para llenar los valores faltantes, lo que lo hace simple y rápido, pero con riesgo de cambiar la distribución de los datos.\n","\n","a- a.Imputación por la media o la mediana\n","\n","Cuándo usarla: Cuando los datos faltantes son numéricos y la variable no tiene sesgo extremo. Ventaja: Simple y rápido de implementar. Desventaja: Puede alterar la distribución de los datos si hay muchos valores faltantes, ya que todos se reemplazan por el mismo valor, reduciendo la variabilidad."],"metadata":{"id":"FH4qw-7vAFed"}},{"cell_type":"code","source":[],"metadata":{"id":"0ej7Z6K2LeDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#a.Imputación por la media\n","#Si el valor de la variable es numérico continuo,imputar con media o mediana es común. Si hay outliers, la mediana es preferible.\n","#Si los valores de EeccNac_T07 son aproximadamente simétricos o no tienen demasiados valores atípicos, puedes imputar los nulos con la media de la variable.\n","mean_value = df_regresion['EeccNac_T07'].mean()\n","df_regresion['EeccNac_T07'].fillna(mean_value, inplace=True)\n","#distribución normal, pero ten cuidado si hay valores extremos que puedan afectar la media."],"metadata":{"id":"1xXAcmOBAUmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Imputar con la Mediana\n","#La mediana es una mejor opción cuando los datos tienen valores extremos o están sesgados, ya que es menos sensible a los valores atípicos.\n","median_value = df_regresion['EeccNac_T07'].median()\n","df_regresion['EeccNac_T07'].fillna(median_value, inplace=True)\n"],"metadata":{"id":"M7eqVt_JKLk5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m6vDcOfeKTbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#b. Imputación por una constante**\n","df['Renta'].fillna(0, inplace=True)  # Reemplazar faltantes con una constante\n"],"metadata":{"id":"KGZtBVIOAMIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Imputación por regresión**\n","\n","#Cuando hay** correlación fuerte** entre las variables, y se puede estimar el valor de una variable faltante usando otras variables.\n","# Preserva la distribución de los datos mejor que la media o mediana.\n","\n","from sklearn.linear_model import LinearRegression\n","\n","# Datos donde no hay nulos\n","complete_data = df.dropna(subset=['Renta'])\n","\n","# Modelar la renta en función de otras variables\n","X = complete_data[['Edad', 'Antiguedad']]\n","y = complete_data['Renta']\n","reg = LinearRegression().fit(X, y)\n","\n","# Predecir los valores faltantes\n","missing_renta = df[df['Renta'].isna()]\n","df.loc[df['Renta'].isna(), 'Renta'] = reg.predict(missing_renta[['Edad', 'Antiguedad']])\n"],"metadata":{"id":"MFEKq9SPAgL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#d. Imputación con k-Nearest Neighbors (KNN) o Hot-Deck. Cuandono hay correlación entre las variables, pero hay vecinos cercanos que pueden servir como referencia para imputar los datos.\n","#El K-Nearest Neighbors Imputer es una técnica más avanzada que usa la similitud entre registros para imputar los valores faltantes. Este método puede ser útil si los datos tienen correlaciones entre múltiples variables.\n","#Ventaja: Mantiene patrones locales en los datos y es más preciso que la media o mediana.\n","from sklearn.impute import KNNImputer\n","\n","imputer = KNNImputer(n_neighbors=5)\n","df_imputed = imputer.fit_transform(df[['Edad', 'Renta', 'Antiguedad']])\n","\n"],"metadata":{"id":"riitdK3wAgIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import KNNImputer\n","\n","imputer = KNNImputer(n_neighbors=5)\n","df_regresion[['EeccNac_T07']] = imputer.fit_transform(df_regresion[['EeccNac_T07']])\n"],"metadata":{"id":"FKGMhAzmLW92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imputar con la Moda:Si la variable tiene valores que son más categóricos o tienes una cantidad limitada de valores posibles, puedes imputar con la moda (el valor más frecuente)\n","mode_value = df_regresion['EeccNac_T07'].mode()[0]\n","df_regresion['EeccNac_T07'].fillna(mode_value, inplace=True)\n"],"metadata":{"id":"IOZ3V0IvAgFg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Escalado de características**\n","###**Normalización:**\n","\n","Ajusta los datos para que se encuentren en un rango específico, generalmente entre 0 y 1.\n","Utiliza MinMaxScaler para las columnas con distribución no normal.\n","\n","###**Estandarización**:\n","\n","Ajusta los datos para que tengan una media de 0 y una desviación estándar de 1.\n","Utiliza StandardScaler para las columnas con distribución normal."],"metadata":{"id":"boTDtBQh9BpB"}},{"cell_type":"code","source":[],"metadata":{"id":"xGUwHokYAgC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tLolc1fhAgAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FASE 4 MODELAMIENTO"],"metadata":{"id":"be0uCAZ8dik6"}},{"cell_type":"code","source":["#Correlacion df_categorica\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#Calcular la matriz de correlación\n","corr_matrix = df_categorica.drop('target', axis=1).corr()\n","\n","#Graficar la matriz de correlación con un mapa de calor\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Matriz de Correlación de los Features')\n","plt.show()\n","#Correlación entre cada característica y el target\n","corr_with_target = df_categorica.corr()['target'].sort_values(ascending=False)\n","print(corr_with_target)\n","sns.pairplot(df_categorica, diag_kind='kde')\n","plt.show()"],"metadata":{"id":"GIP3h8fvd3bh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Clasificación KNN\n","#Librerías\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from imblearn.over_sampling import SMOTE  # Importar SMOTE\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc\n","\n","#Preparación de los datos\n","X = df_categorica.drop('target', axis=1)  #Características\n","y = df_categorica['target']                 #Variable objetivo\n","\n","#División de los datos en entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","#Aplicar SMOTE en el conjunto de entrenamiento\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n","\n","#Normalización de los datos\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_resampled)\n","X_test_scaled = scaler.transform(X_test)\n","\n","#Definición del modelo KNN\n","knn = KNeighborsClassifier(\n","    n_neighbors=3,\n","    weights='uniform',\n","    algorithm='auto',\n","    metric='minkowski',\n","    leaf_size=30,\n","    n_jobs=-1\n",")\n","\n","#Validación cruzada\n","scores = cross_val_score(knn, X_train_scaled, y_train_resampled, cv=5)  # Usar y_train_resampled\n","print(\"Cross-validation scores:\", scores)\n","print(\"Average cross-validation score:\", np.mean(scores))\n","print(\"Standard deviation of cross-validation scores:\", np.std(scores))\n","\n","#Entrenamiento del modelo\n","knn.fit(X_train_scaled, y_train_resampled)\n","\n","#Predicciones\n","y_pred = knn.predict(X_test_scaled)\n","\n","#Evaluación del modelo\n","print(\"Matriz de confusión:\")\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\nReporte de clasificación:\")\n","print(classification_report(y_test, y_pred))\n","print(\"\\nPrecisión del modelo en el conjunto de prueba:\", accuracy_score(y_test, y_pred))\n","\n","#ROC\n","y_scores = knn.predict_proba(X_test_scaled)[:, 1]\n","fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n","roc_auc = auc(fpr, tpr)\n","plt.figure(figsize=(10, 6))\n","plt.plot(fpr, tpr, color='blue', label=f'Curva ROC (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Línea diagonal\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Tasa de Falsos Positivos')\n","plt.ylabel('Tasa de Verdaderos Positivos')\n","plt.title('Curva ROC')\n","plt.legend(loc='lower right')\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"WicRU36odo6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Clasificacion Random Forest\n","#Librerías\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from imblearn.over_sampling import SMOTE\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc\n","\n","#Preparación de los datos\n","X = df_categorica.drop('target', axis=1)  #Características\n","y = df_categorica['target']               #Variable objetivo\n","\n","#División de los datos en entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","#Aplicar SMOTE en el conjunto de entrenamiento\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n","\n","#Normalización de los datos\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_resampled)\n","X_test_scaled = scaler.transform(X_test)\n","\n","#Definición del modelo Random Forest\n","rf = RandomForestClassifier(\n","    n_estimators=100,        # Número de árboles en el bosque\n","    max_features='sqrt',     # Número máximo de características para dividir un nodo\n","    max_depth=None,          # Sin límite de profundidad\n","    min_samples_split=2,     # Mínimo número de muestras para dividir un nodo\n","    min_samples_leaf=1,      # Mínimo número de muestras en un nodo hoja\n","    bootstrap=True,          # Usar muestras bootstrap\n","    random_state=42,         # Reproducibilidad\n","    n_jobs=-1                # Usar todos los núcleos disponibles\n",")\n","\n","#Validación cruzada con búsqueda de hiperparámetros\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_features': ['sqrt', 'log2'],\n","    'max_depth': [10, 20, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n","grid_search.fit(X_train_scaled, y_train_resampled)\n","\n","#Mejor estimador\n","best_rf = grid_search.best_estimator_\n","print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n","\n","#Entrenamiento del modelo con los mejores hiperparámetros\n","best_rf.fit(X_train_scaled, y_train_resampled)\n","\n","#Predicciones\n","y_pred = best_rf.predict(X_test_scaled)\n","\n","#Evaluación del modelo\n","print(\"Matriz de confusión:\")\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\nReporte de clasificación:\")\n","print(classification_report(y_test, y_pred))\n","print(\"\\nPrecisión del modelo en el conjunto de prueba:\", accuracy_score(y_test, y_pred))\n","\n","#ROC\n","y_scores = best_rf.predict_proba(X_test_scaled)[:, 1]\n","fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n","roc_auc = auc(fpr, tpr)\n","plt.figure(figsize=(10, 6))\n","plt.plot(fpr, tpr, color='blue', label=f'Curva ROC (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Tasa de Falsos Positivos')\n","plt.ylabel('Tasa de Verdaderos Positivos')\n","plt.title('Curva ROC')\n","plt.legend(loc='lower right')\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"uI-lsRS0d0VL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rWiygXJ5do_g"},"execution_count":null,"outputs":[]}]}